{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !export CUDA_VISIBLE_DEVICES=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.set_device(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TXk4O_B___rO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.16.2\n"
          ]
        }
      ],
      "source": [
        "#! pip install datasets transformers rouge-score nltk torch numpy matplotlib      # << Uncomment to install packages\n",
        "import transformers\n",
        "print(transformers.__version__)     # Should be >= 4.11.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a summarization task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [HuggingFace Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n",
        "\n",
        "![Widget inference on a summarization task](https://github.com/huggingface/notebooks/blob/master/examples/images/summarization.png?raw=1)\n",
        "\n",
        "We will see how to easily load the dataset for this task using HuggingFace Datasets and how to fine-tune a model on it using the `Trainer` API.\n",
        "\n",
        "This tutorial is draws from the Huggingface Summarization Tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [HuggingFace Datasets](https://github.com/huggingface/datasets) library to download the data we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset`. The Datasets library is a great resource for conveniently working with common datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "outputId": "6564e8c9-ec95-4527-84ba-b83e3b5a2cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews'] 2833\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset xsum (/vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0566ee8295648808bd17b579a8303c0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['train', 'validation', 'test']) \n",
            " Dataset({\n",
            "    features: ['document', 'summary', 'id'],\n",
            "    num_rows: 204045\n",
            "})\n",
            "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.', 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.', 'id': '35232142'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "\n",
        "# Can see all possible datasets as follows\n",
        "all_datasets = list_datasets()\n",
        "print(all_datasets[:10], len(all_datasets))\n",
        "\n",
        "raw_datasets = load_dataset(\"xsum\")\n",
        "\n",
        "# Each dataset has a train, val and test split\n",
        "print(raw_datasets.keys(), '\\n', raw_datasets['train'])\n",
        "\n",
        "# An XSum sample looks as follows\n",
        "print(raw_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "### EDA\n",
        "\n",
        "To get a sense of what the data looks like, we will write a function to display some random elements. We will also perform some basic EDA (exploratory data analysis) by computing the mean and standard deviation token counts for the source and target documents.\n",
        "\n",
        "1. Complete `show_random_elements` below. This should display a table containing `num_examples` samples from the specified `dataset`, with fields `Souce Document`, `Target Document` and `Document ID`\n",
        "2. EDA: \n",
        " - Tokenize the document (the best option would be to use the tokenizer but here we will just split on spaces) and print the mean count and standard deviation for source and target documents for each dataset\n",
        " - Plot histograms of the source and target token counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1008x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    ## YOUR CODE HERE ##\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "\n",
        "def get_token_counts(dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    src_counts = [len(x.split()) for x in dataset['document']]\n",
        "    tgt_counts = [len(x.split()) for x in dataset['summary']]\n",
        "    return src_counts, tgt_counts\n",
        "\n",
        "\n",
        "def token_counts_summary(raw_dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    for name,dataset in raw_dataset.items():\n",
        "        src_counts, tgt_counts = get_token_counts(dataset)\n",
        "\n",
        "        mean_cnt = np.mean(src_counts)\n",
        "        std_cnt = np.std(src_counts)\n",
        "        print(f'Source documents from dataset: {name:10}\\tMean tokens: {mean_cnt:.2f}\\tStd tokens: {std_cnt:.2f}')\n",
        "\n",
        "        mean_cnt = np.mean(tgt_counts)\n",
        "        std_cnt = np.std(tgt_counts)\n",
        "        print(f'Target documents from dataset: {name:10}\\tMean tokens: {std_cnt:.2f}\\tStd tokens: {std_cnt:.2f}')\n",
        "\n",
        "\n",
        "def plot_token_counts(dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    src_counts, tgt_counts = get_token_counts(dataset)\n",
        "    f, (ax0, ax1) = plt.subplots(1,2, figsize=(14,8))\n",
        "    ax0.hist(src_counts, bins=20)\n",
        "    ax1.hist(tgt_counts, bins=20)\n",
        "    ax0.set_title(\"Source Token Count Distribution\")\n",
        "    ax1.set_title(\"Target Token Count Distribution\")\n",
        "\n",
        "\n",
        "# show_random_elements(raw_datasets[\"val\"])\n",
        "\n",
        "plot_token_counts(raw_datasets[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "### Loading the metric\n",
        "\n",
        "To evaluate our model's performance, we will use the ROUGE summarization metric. This is provided natively within the dataset library and can be loaded similarly to how we loaded the dataset above. **Note** this is a big advantage in practise as i) metrics can be fiddly to implement manually and ii) difficult to align completely across implmentations as decisions like lemmatization, tokenization and punctation-handling can create large discrepancies in scores.\n",
        "\n",
        "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6XN1Rq0aIrJC",
        "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666), mid=Score(precision=0.8333333333333333, recall=0.8333333333333333, fmeasure=0.8333333333333333), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.5, recall=0.5, fmeasure=0.5), mid=Score(precision=0.75, recall=0.75, fmeasure=0.75), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666), mid=Score(precision=0.8333333333333333, recall=0.8333333333333333, fmeasure=0.8333333333333333), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666), mid=Score(precision=0.8333333333333333, recall=0.8333333333333333, fmeasure=0.8333333333333333), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"rouge\")\n",
        "\n",
        "# help(metric)      # << Uncomment to see more about the ROUGE eval metric\n",
        "\n",
        "# Try it out below\n",
        "# fake_preds = ## YOUR CODE HERE\n",
        "# fake_labels = ## YOUR CODE HERE\n",
        "## COMPUTE METRIC HERE ## \n",
        "\n",
        "fake_preds = [\"i love nlp\", \"nlp is great\"]\n",
        "fake_labels = [\"i love nlp\", \"nlp is fun\"]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "We will proprocess the data using the Huggingface `Tokenizer`. This tokenizes and indexes the inputs and put it in a format the model expects. It also generate the other inputs that the model requires.\n",
        "\n",
        "-------------- Question ---------------------\n",
        "\n",
        "1. Each Huggingface model has a paired tokenizer. Why is it important to use the appropriate tokenizer?\n",
        "\n",
        "Answer: \n",
        "Otherwise the tokenizer vocabulary may not be aligned with the model's embedding matrics. This could result in i) incorrect token embeddings (as token indices are incorrect), ii) index errors (if the vocbulary shrinks) iii) incorrect sub-word tokenization (e.g. if the model was trained on sentencepiece but the tokenizer uses byte-pair-encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"t5-small\"       # Can find more options at https://huggingface.co/models?sort=downloads&search=t5    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the tokenizer\n",
        "\n",
        "Try out the tokenizer in the cell below.\n",
        "\n",
        "---------------- Question ---------------\n",
        "\n",
        "1. The tokenizer output has different fields for different tokenizers. \n",
        "\n",
        "i) Confirm this by writing code below to instantiate a BERT tokenizer and comparing the outputs across the two tokenizers of when tokenizing a string. You may find [this page](https://huggingface.co/models) helpful.\n",
        "\n",
        "ii) Why does the BERT tokenizer have an additional field to the T5 tokenizer?\n",
        "\n",
        "Answer: most models were trained to recognize sequences defined using special tokens, e.g. [CLS] seq1 [SEP] seq2 [SEP] is a common encoding pattern for tasks involving two sequences. BERT was trained using an additional `token_type_ids` field to which is a binary mask flagging to explicitly differentiate between the two sequences. BERT will likely perform poorly evaluated without the type ids. See [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested to find out more about tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 333, 445, 6892, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2057, 2293, 17953, 2361, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test the tokenizer below\n",
        "print(tokenizer(\"We love NLP!\"))\n",
        "# print(tokenizer(## YOUR CODE HERE))\n",
        "\n",
        "## CODE FOR 1.i) HERE ##\n",
        "tokenizer_ = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer_('We love NLP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional tokenizer functionalities:\n",
        "\n",
        "- The tokenizer can be fed a list of strings\n",
        "- When tokenizing the target documents, using `tokenizer.as_target_tokenizer()` to ensure the target receives the appropriate special tokens (although here the source and target are tokenized identically)\n",
        "- We can convert back from ids to tokens by using `tokenizer.convert_ids_to_tokens()`\n",
        "\n",
        "--------------- Question --------------\n",
        "\n",
        "1. The output of `tokenizer.convert_ids_to_tokens()` is different from the inital string. Why is this?\n",
        "\n",
        "Answer: because of subword tokenization. Subwords are identified by sub-words not having an underscore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TIaPIvgF__rs",
        "outputId": "3e87d343-a4a5-4d72-b147-9cd3d5f8efc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
            "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
            "['▁We', '▁love', '▁N', 'LP', '!', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(tokenizer(\"We love NLP!\")['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "T5 was trained within a multitask framework such that it can perform multiple tasks out-of-the-box. We prefix the inputs with \"summarize: \" to prompt the model to deliver the correct outputs.\n",
        "\n",
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset.\n",
        "\n",
        "Complete the `preprocess_function` below to tokenize the text (**hint**: don't forget prefix or the context manager ;) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \" if model_checkpoint.startswith(\"t5-\") else \"\"\n",
        "\n",
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    ## YOUR CODE HERE ##\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-b70jh26IrJS",
        "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[21603, 10, 37, 423, 583, 13, 1783, 16, 20126, 16496, 6, 80, 13, 8, 844, 6025, 4161, 6, 19, 341, 271, 14841, 5, 7057, 161, 19, 4912, 16, 1626, 5981, 11, 186, 7540, 16, 1276, 15, 2296, 7, 5718, 2367, 14621, 4161, 57, 4125, 387, 5, 15059, 7, 30, 8, 4653, 4939, 711, 747, 522, 17879, 788, 12, 1783, 44, 8, 15763, 6029, 1813, 9, 7472, 5, 1404, 1623, 11, 5699, 277, 130, 4161, 57, 18368, 16, 20126, 16496, 227, 8, 2473, 5895, 15, 147, 89, 22411, 139, 8, 1511, 5, 1485, 3271, 3, 21926, 9, 472, 19623, 5251, 8, 616, 12, 15614, 8, 1783, 5, 37, 13818, 10564, 15, 26, 3, 9, 3, 19513, 1481, 6, 18368, 186, 1328, 2605, 30, 7488, 1887, 3, 18, 8, 711, 2309, 9517, 89, 355, 5, 3966, 1954, 9233, 15, 6, 113, 293, 7, 8, 16548, 13363, 106, 14022, 84, 47, 14621, 4161, 6, 243, 255, 228, 59, 7828, 8, 1249, 18, 545, 11298, 1773, 728, 8, 8347, 1560, 5, 611, 6, 255, 243, 72, 1709, 1528, 161, 228, 43, 118, 4006, 91, 12, 766, 8, 3, 19513, 1481, 410, 59, 5124, 5, 96, 196, 17, 19, 1256, 68, 27, 103, 317, 132, 19, 78, 231, 23546, 21, 970, 51, 89, 2593, 11, 8, 2504, 189, 3, 18, 11, 27, 3536, 3653, 24, 3, 18, 68, 34, 19, 966, 114, 62, 31, 60, 23708, 42, 11821, 976, 255, 243, 5, 96, 11880, 164, 59, 36, 1176, 68, 34, 19, 2361, 82, 3503, 147, 8, 336, 360, 477, 5, 96, 17891, 130, 25, 59, 1065, 12, 199, 178, 3, 9, 720, 72, 116, 8, 6337, 11, 8, 6196, 5685, 7, 141, 2767, 91, 4609, 7940, 6, 3, 9, 8347, 5685, 3048, 16, 286, 640, 8, 17600, 7, 250, 13, 8, 3917, 3412, 5, 1276, 15, 2296, 7, 47, 14621, 1560, 57, 982, 6, 13233, 53, 3088, 12, 4277, 72, 13613, 7, 16, 8, 616, 5, 12580, 17600, 7, 2063, 65, 474, 3, 9, 570, 30, 165, 475, 13, 8, 7540, 6025, 4161, 11, 3863, 43, 118, 3, 19492, 59, 12, 9751, 12493, 3957, 5, 37, 16117, 3450, 31, 7, 21108, 12580, 2488, 5104, 11768, 1306, 47, 16, 1626, 5981, 30, 2089, 12, 217, 8, 1419, 166, 609, 5, 216, 243, 34, 47, 359, 12, 129, 8, 8347, 1711, 515, 269, 68, 3, 9485, 3088, 12, 1634, 95, 8, 433, 5, 96, 196, 47, 882, 1026, 3, 9, 1549, 57, 8, 866, 13, 1783, 24, 65, 118, 612, 976, 3, 88, 243, 5, 96, 14116, 34, 19, 842, 18, 18087, 21, 151, 113, 43, 118, 5241, 91, 13, 70, 2503, 11, 8, 1113, 30, 1623, 535, 216, 243, 34, 47, 359, 24, 96, 603, 5700, 342, 2245, 121, 130, 1026, 12, 1822, 8, 844, 167, 9930, 11, 3, 9, 964, 97, 3869, 474, 16, 286, 21, 8347, 9793, 1390, 5, 2114, 25, 118, 4161, 57, 18368, 16, 970, 51, 89, 2593, 11, 10987, 32, 1343, 42, 8, 17600, 7, 58, 8779, 178, 81, 39, 351, 13, 8, 1419, 11, 149, 34, 47, 10298, 5, 8601, 178, 30, 142, 40, 157, 12546, 5, 15808, 1741, 115, 115, 75, 5, 509, 5, 1598, 42, 146, 51, 89, 2593, 1741, 115, 115, 75, 5, 509, 5, 1598, 5, 1], [21603, 10, 71, 1472, 6196, 877, 326, 44, 8, 9108, 86, 29, 16, 6000, 1887, 44, 81, 11484, 10, 1755, 272, 4209, 30, 1856, 11, 2554, 130, 1380, 12, 1175, 8, 1595, 5, 282, 79, 3, 9094, 1067, 79, 1509, 8, 192, 14264, 6, 3, 16669, 596, 18, 969, 18, 1583, 16, 8, 443, 2447, 6, 3, 35, 6106, 19565, 57, 12314, 7, 5, 555, 13, 8, 1552, 1637, 19, 45, 3434, 6, 8, 119, 45, 1473, 11, 14441, 5, 94, 47, 70, 166, 706, 16, 5961, 5316, 5, 37, 2535, 13, 80, 13, 8, 14264, 243, 186, 13, 8, 9234, 141, 646, 525, 12770, 7, 30, 1476, 11, 175, 141, 118, 10932, 5, 2867, 1637, 43, 13666, 3709, 11210, 11, 56, 1731, 70, 1552, 13, 8, 3457, 4939, 865, 145, 79, 141, 4355, 5, 5076, 43, 3958, 15, 26, 21, 251, 81, 8, 3211, 5, 86, 7, 102, 1955, 24723, 243, 10, 96, 196, 17, 3475, 38, 713, 8, 1472, 708, 365, 80, 13, 8, 14264, 274, 16436, 12, 8, 511, 5, 96, 27674, 8, 2883, 1137, 19, 341, 365, 4962, 6, 34, 19, 816, 24, 8, 1472, 47, 708, 24067, 535, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[7433, 18, 413, 2673, 33, 6168, 640, 8, 12580, 17600, 7, 11, 970, 51, 89, 2593, 11, 10987, 32, 1343, 227, 18368, 2953, 57, 16133, 4937, 5, 1], [2759, 8548, 14264, 43, 118, 10932, 57, 1472, 16, 3, 9, 18024, 1584, 739, 3211, 16, 27874, 690, 2050, 5, 1]]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test it using the below call\n",
        "preprocess_function(raw_datasets['train'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "This function can be applied to all our datasets using the `map` method of our `dataset` object. The results are automatically cached to avoid spending time on this step the next time you run your notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-5ebb2d6a3db1bb19.arrow\n",
            "Loading cached processed dataset at /vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-933a7931c8603918.arrow\n",
            "Loading cached processed dataset at /vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-6ce473e7288e00a1.arrow\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_function, \n",
        "    batched=True            # This employs multithreading to speed up tokenization\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using T5 out-of-the-box\n",
        "\n",
        "In the lecture slides you were shown the below image:\n",
        "\n",
        "![T5 img](figs/t5.png)\n",
        "\n",
        "Here, we will experiment with some of T5's capabilities without pre-training. First, we will download the model using `AutoModelForSeq2SeqLM` class using the `from_pretrained` method (this caches the model for us).\n",
        "\n",
        "To illustrate the impact of the prompt, we will try T5 using two different prompts using the same string. Note that we are using the small version without fine-tuning so the results may not be great.\n",
        "\n",
        "### Your task\n",
        "Complete the `tok_and_gen` function below. This requires the following steps:\n",
        "- Tokenize the inputs\n",
        "- Generate output ids (hint: using model.generate. Google this for more details)\n",
        "- Convert the ids to string\n",
        "- Print the string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using prefix translate English to German: :  ['Die künstliche allgemeine Intelligenz (AGI) ist die']\n",
            "Using prefix summarize: :  ['artificial intelligence is the hypothetical ability of an intelligent agent to understand or learn any intellectual task that ']\n"
          ]
        }
      ],
      "source": [
        "input_str = 'Artificial general intelligence (AGI) is the hypothetical ability of an intelligent agent to understand or learn any intellectual task that a human being can.[1] It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies.'\n",
        "\n",
        "prefixes = [\"translate English to German: \", \"summarize: \"]     # What happens if we make up a prompt? Why?\n",
        "\n",
        "def tok_and_gen(model, tokenizer, prefix, input):\n",
        "    ## YOUR CODE HERE ##\n",
        "    inputs = tokenizer(prefix + input, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"].cuda(),\n",
        "        attention_mask=inputs[\"attention_mask\"].cuda(),\n",
        "        do_sample=False,  # disable sampling to test if batching affects output\n",
        "    )\n",
        "\n",
        "    print(f'Using prefix {task_prefix}: ', tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n",
        "\n",
        "\n",
        "for task_prefix in prefixes:\n",
        "    tok_and_gen(model, tokenizer, task_prefix, input_str)    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready and we have played around with our model, we can fine-tune it. \n",
        "\n",
        "HuggingFace provides an API for training a seq2seq model: the `Seq2SeqTrainer`. To instantiate this, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires a folder name for saving checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "X8WyIhF-__sL"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-xsum\",     # Ouptut folder\n",
        "    # Eval strategy\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    # Could alternatively be the following to eval every epoch:\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "\n",
        "    # LR. Should be small (<1e-4)\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # Batch size during training and eval\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "\n",
        "    # Limits the number of models saved during training. Important to prevent memory clogging up!\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Properly generate summaries during eval\n",
        "    predict_with_generate=True,\n",
        "    \n",
        "    # Mixed precision training (speeds up training - see Nvidia-apex for more details)\n",
        "    fp16=True,\n",
        "\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_tMJC9gy__sL"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdzABDVcIrJg"
      },
      "source": [
        "We can now finetune our model by just calling the `train` method. This will take a while as summarization is an expensive task in general and the dataset is medium sized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "uNx5pyRlIrJh",
        "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec",
        "scrolled": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, document, summary.\n",
            "/vol/bitbucket/aeg19/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/vol/bitbucket/aeg19/anaconda3/envs/mynlplab/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
            "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
            "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
            "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
            "    environment variable.\n",
            "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
            "***** Running training *****\n",
            "  Num examples = 204045\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 4\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 40\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5102\n",
            "/vol/bitbucket/aeg19/anaconda3/envs/mynlplab/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='201' max='5102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 201/5102 08:23 < 3:26:42, 0.40 it/s, Epoch 0.04/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>No log</td>\n",
              "      <td>2.823943</td>\n",
              "      <td>19.649700</td>\n",
              "      <td>3.485200</td>\n",
              "      <td>15.331900</td>\n",
              "      <td>15.397500</td>\n",
              "      <td>18.600600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='121' max='284' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [121/284 02:08 < 02:54, 0.94 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, document, summary.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 11332\n",
            "  Batch size = 40\n",
            "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, document, summary.\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 11332\n",
            "  Batch size = 40\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_35818/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1440\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1441\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1563\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, max_length, num_beams)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_max_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_beams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_num_beams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     def predict(\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2208\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2209\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2210\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mgeneration_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain_input_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         generated_tokens = self.model.generate(\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0mgeneration_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;31m# 10. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1174\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   1470\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1615\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1616\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1617\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1618\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/anaconda3/envs/mynlplab/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mencoder_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Summarization",
      "provenance": []
    },
    "interpreter": {
      "hash": "65bdeda9c857d200db339bc1461d10a02d342df00d855a84b263b8efa6d3dc02"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('NLPlabs': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
