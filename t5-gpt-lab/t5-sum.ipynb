{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXk4O_B___rO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.16.2\n"
          ]
        }
      ],
      "source": [
        "#! pip install datasets transformers rouge-score nltk torch numpy matplotlib      # << Uncomment to install packages\n",
        "import transformers\n",
        "print(transformers.__version__)     # Should be >= 4.11.0\n",
        "import torch\n",
        "# !export CUDA_VISIBLE_DEVICES=0        # Useful for running outside of colabs\n",
        "# torch.cuda.set_device(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJBSTyZIrIb"
      },
      "source": [
        "# Fine-tuning a model on a summarization task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTCFado4IrIc"
      },
      "source": [
        "In this notebook, we will see how to fine-tune one of the [HuggingFace Transformers](https://github.com/huggingface/transformers) model for a summarization task. We will use the [XSum dataset](https://arxiv.org/pdf/1808.08745.pdf) (for extreme summarization) which contains BBC articles accompanied with single-sentence summaries.\n",
        "\n",
        "![Widget inference on a summarization task](https://github.com/huggingface/notebooks/blob/master/examples/images/summarization.png?raw=1)\n",
        "\n",
        "We will see how to easily load the dataset for this task using HuggingFace Datasets and how to fine-tune a model on it using the `Trainer` API.\n",
        "\n",
        "This tutorial is draws from the Huggingface Summarization Tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whPRbBNbIrIl"
      },
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7QYTpxXIrIl"
      },
      "source": [
        "We will use the [HuggingFace Datasets](https://github.com/huggingface/datasets) library to download the data we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset`. The Datasets library is a great resource for conveniently working with common datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IreSlFmlIrIm",
        "outputId": "6564e8c9-ec95-4527-84ba-b83e3b5a2cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews'] 2795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset xsum (/vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6556e0adf684499ebbe7d891d7a69790",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['train', 'validation', 'test']) \n",
            " Dataset({\n",
            "    features: ['document', 'summary', 'id'],\n",
            "    num_rows: 204045\n",
            "})\n",
            "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.', 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.', 'id': '35232142'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "\n",
        "# Can see all possible datasets as follows\n",
        "all_datasets = list_datasets()\n",
        "print(all_datasets[:10], len(all_datasets))\n",
        "\n",
        "raw_datasets = load_dataset(\"xsum\")\n",
        "\n",
        "# Each dataset has a train, val and test split\n",
        "print(raw_datasets.keys(), '\\n', raw_datasets['train'])\n",
        "\n",
        "# An XSum sample looks as follows\n",
        "print(raw_datasets[\"train\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHUmphG3IrI3"
      },
      "source": [
        "### EDA\n",
        "\n",
        "To get a sense of what the data looks like, we will write a function to display some random elements. We will also perform some basic EDA (exploratory data analysis) by computing the mean and standard deviation token counts for the source and target documents.\n",
        "\n",
        "1. Complete `show_random_elements` below. This should display a table containing `num_examples` samples from the specified `dataset`, with fields `Souce Document`, `Target Document` and `Document ID`\n",
        "2. EDA: \n",
        " - Tokenize the document (the best option would be to use the tokenizer but here we will just split on spaces) and print the mean count and standard deviation for source and target documents for each dataset\n",
        " - Plot histograms of the source and target token counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3j8APAoIrI3"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1008x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import datasets\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    ## YOUR CODE HERE ##\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "    \n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, datasets.ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    display(HTML(df.to_html()))\n",
        "\n",
        "\n",
        "def get_token_counts(dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    src_counts = [len(x.split()) for x in dataset['document']]\n",
        "    tgt_counts = [len(x.split()) for x in dataset['summary']]\n",
        "    return src_counts, tgt_counts\n",
        "\n",
        "\n",
        "def token_counts_summary(raw_dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    for name,dataset in raw_dataset.items():\n",
        "        src_counts, tgt_counts = get_token_counts(dataset)\n",
        "\n",
        "        mean_cnt = np.mean(src_counts)\n",
        "        std_cnt = np.std(src_counts)\n",
        "        print(f'Source documents from dataset: {name:10}\\tMean tokens: {mean_cnt:.2f}\\tStd tokens: {std_cnt:.2f}')\n",
        "\n",
        "        mean_cnt = np.mean(tgt_counts)\n",
        "        std_cnt = np.std(tgt_counts)\n",
        "        print(f'Target documents from dataset: {name:10}\\tMean tokens: {std_cnt:.2f}\\tStd tokens: {std_cnt:.2f}')\n",
        "\n",
        "\n",
        "def plot_token_counts(dataset):\n",
        "    ## YOUR CODE HERE ##\n",
        "    src_counts, tgt_counts = get_token_counts(dataset)\n",
        "    f, (ax0, ax1) = plt.subplots(1,2, figsize=(14,8))\n",
        "    ax0.hist(src_counts, bins=20)\n",
        "    ax1.hist(tgt_counts, bins=20)\n",
        "    ax0.set_title(\"Source Token Count Distribution\")\n",
        "    ax1.set_title(\"Target Token Count Distribution\")\n",
        "\n",
        "\n",
        "# show_random_elements(raw_datasets[\"val\"])\n",
        "\n",
        "plot_token_counts(raw_datasets[\"validation\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAWdqcUBIrJC"
      },
      "source": [
        "### Loading the metric\n",
        "\n",
        "To evaluate our model's performance, we will use the ROUGE summarization metric. This is provided natively within the dataset library and can be loaded similarly to how we loaded the dataset above. **Note** this is a big advantage in practise as i) metrics can be fiddly to implement manually and ii) difficult to align completely across implmentations as decisions like lemmatization, tokenization and punctation-handling can create large discrepancies in scores.\n",
        "\n",
        "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6XN1Rq0aIrJC",
        "outputId": "a4405435-a8a9-41ff-9f79-a13077b587c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666), mid=Score(precision=0.8333333333333333, recall=0.8333333333333333, fmeasure=0.8333333333333333), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.5, recall=0.5, fmeasure=0.5), mid=Score(precision=0.75, recall=0.75, fmeasure=0.75), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666), mid=Score(precision=0.8333333333333333, recall=0.8333333333333333, fmeasure=0.8333333333333333), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.6666666666666666, recall=0.6666666666666666, fmeasure=0.6666666666666666), mid=Score(precision=0.8333333333333333, recall=0.8333333333333333, fmeasure=0.8333333333333333), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_metric\n",
        "metric = load_metric(\"rouge\")\n",
        "\n",
        "# help(metric)      # << Uncomment to see more about the ROUGE eval metric\n",
        "\n",
        "# Try it out below\n",
        "# fake_preds = ## YOUR CODE HERE\n",
        "# fake_labels = ## YOUR CODE HERE\n",
        "## COMPUTE METRIC HERE ## \n",
        "\n",
        "fake_preds = [\"i love nlp\", \"nlp is great\"]\n",
        "fake_labels = [\"i love nlp\", \"nlp is fun\"]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9qywopnIrJH"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVx71GdAIrJH"
      },
      "source": [
        "We will proprocess the data using the Huggingface `Tokenizer`. This tokenizes and indexes the inputs and put it in a format the model expects. It also generate the other inputs that the model requires.\n",
        "\n",
        "-------------- Question ---------------------\n",
        "\n",
        "1. Each Huggingface model has a paired tokenizer. Why is it important to use the appropriate tokenizer?\n",
        "\n",
        "Answer: \n",
        "Otherwise the tokenizer vocabulary may not be aligned with the model's embedding matrics. This could result in i) incorrect token embeddings (as token indices are incorrect), ii) index errors (if the vocbulary shrinks) iii) incorrect sub-word tokenization (e.g. if the model was trained on sentencepiece but the tokenizer uses byte-pair-encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eXNLu_-nIrJI"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"t5-small\"       # Can find more options at https://huggingface.co/models?sort=downloads&search=t5    \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the tokenizer\n",
        "\n",
        "Try out the tokenizer in the cell below.\n",
        "\n",
        "---------------- Question ---------------\n",
        "\n",
        "1. The tokenizer output has different fields for different tokenizers. \n",
        "\n",
        "i) Confirm this by writing code below to instantiate a BERT tokenizer and comparing the outputs across the two tokenizers of when tokenizing a string. You may find [this page](https://huggingface.co/models) helpful.\n",
        "\n",
        "ii) Why does the BERT tokenizer have an additional field to the T5 tokenizer?\n",
        "\n",
        "Answer: most models were trained to recognize sequences defined using special tokens, e.g. [CLS] seq1 [SEP] seq2 [SEP] is a common encoding pattern for tasks involving two sequences. BERT was trained using an additional `token_type_ids` field to which is a binary mask flagging to explicitly differentiate between the two sequences. BERT will likely perform poorly evaluated without the type ids. See [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested to find out more about tokenization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 333, 445, 6892, 55, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 2057, 2293, 17953, 2361, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test the tokenizer below\n",
        "print(tokenizer(\"We love NLP!\"))\n",
        "# print(tokenizer(## YOUR CODE HERE))\n",
        "\n",
        "## CODE FOR 1.i) HERE ##\n",
        "tokenizer_ = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer_('We love NLP')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Additional tokenizer functionalities:\n",
        "\n",
        "- The tokenizer can be fed a list of strings\n",
        "- When tokenizing the target documents, using `tokenizer.as_target_tokenizer()` to ensure the target receives the appropriate special tokens (although here the source and target are tokenized identically)\n",
        "- We can convert back from ids to tokens by using `tokenizer.convert_ids_to_tokens()`\n",
        "\n",
        "--------------- Question --------------\n",
        "\n",
        "1. The output of `tokenizer.convert_ids_to_tokens()` is different from the inital string. Why is this?\n",
        "\n",
        "Answer: because of subword tokenization. Subwords are identified by sub-words not having an underscore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TIaPIvgF__rs",
        "outputId": "3e87d343-a4a5-4d72-b147-9cd3d5f8efc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
            "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
            "['▁We', '▁love', '▁N', 'LP', '!', '</s>']\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(tokenizer(\"We love NLP!\")['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C0hcmp9IrJQ"
      },
      "source": [
        "T5 was trained within a multitask framework such that it can perform multiple tasks out-of-the-box. We prefix the inputs with \"summarize: \" to prompt the model to deliver the correct outputs.\n",
        "\n",
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset.\n",
        "\n",
        "Complete the `preprocess_function` below to tokenize the text (**hint**: don't forget prefix or the context manager ;) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vc0BSBLIIrJQ"
      },
      "outputs": [],
      "source": [
        "prefix = \"summarize: \" if model_checkpoint.startswith(\"t5-\") else \"\"\n",
        "\n",
        "max_input_length = 1024\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    ## YOUR CODE HERE ##\n",
        "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-b70jh26IrJS",
        "outputId": "acd3a42d-985b-44ee-9daa-af5d944ce1d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [[21603, 10, 37, 423, 583, 13, 1783, 16, 20126, 16496, 6, 80, 13, 8, 844, 6025, 4161, 6, 19, 341, 271, 14841, 5, 7057, 161, 19, 4912, 16, 1626, 5981, 11, 186, 7540, 16, 1276, 15, 2296, 7, 5718, 2367, 14621, 4161, 57, 4125, 387, 5, 15059, 7, 30, 8, 4653, 4939, 711, 747, 522, 17879, 788, 12, 1783, 44, 8, 15763, 6029, 1813, 9, 7472, 5, 1404, 1623, 11, 5699, 277, 130, 4161, 57, 18368, 16, 20126, 16496, 227, 8, 2473, 5895, 15, 147, 89, 22411, 139, 8, 1511, 5, 1485, 3271, 3, 21926, 9, 472, 19623, 5251, 8, 616, 12, 15614, 8, 1783, 5, 37, 13818, 10564, 15, 26, 3, 9, 3, 19513, 1481, 6, 18368, 186, 1328, 2605, 30, 7488, 1887, 3, 18, 8, 711, 2309, 9517, 89, 355, 5, 3966, 1954, 9233, 15, 6, 113, 293, 7, 8, 16548, 13363, 106, 14022, 84, 47, 14621, 4161, 6, 243, 255, 228, 59, 7828, 8, 1249, 18, 545, 11298, 1773, 728, 8, 8347, 1560, 5, 611, 6, 255, 243, 72, 1709, 1528, 161, 228, 43, 118, 4006, 91, 12, 766, 8, 3, 19513, 1481, 410, 59, 5124, 5, 96, 196, 17, 19, 1256, 68, 27, 103, 317, 132, 19, 78, 231, 23546, 21, 970, 51, 89, 2593, 11, 8, 2504, 189, 3, 18, 11, 27, 3536, 3653, 24, 3, 18, 68, 34, 19, 966, 114, 62, 31, 60, 23708, 42, 11821, 976, 255, 243, 5, 96, 11880, 164, 59, 36, 1176, 68, 34, 19, 2361, 82, 3503, 147, 8, 336, 360, 477, 5, 96, 17891, 130, 25, 59, 1065, 12, 199, 178, 3, 9, 720, 72, 116, 8, 6337, 11, 8, 6196, 5685, 7, 141, 2767, 91, 4609, 7940, 6, 3, 9, 8347, 5685, 3048, 16, 286, 640, 8, 17600, 7, 250, 13, 8, 3917, 3412, 5, 1276, 15, 2296, 7, 47, 14621, 1560, 57, 982, 6, 13233, 53, 3088, 12, 4277, 72, 13613, 7, 16, 8, 616, 5, 12580, 17600, 7, 2063, 65, 474, 3, 9, 570, 30, 165, 475, 13, 8, 7540, 6025, 4161, 11, 3863, 43, 118, 3, 19492, 59, 12, 9751, 12493, 3957, 5, 37, 16117, 3450, 31, 7, 21108, 12580, 2488, 5104, 11768, 1306, 47, 16, 1626, 5981, 30, 2089, 12, 217, 8, 1419, 166, 609, 5, 216, 243, 34, 47, 359, 12, 129, 8, 8347, 1711, 515, 269, 68, 3, 9485, 3088, 12, 1634, 95, 8, 433, 5, 96, 196, 47, 882, 1026, 3, 9, 1549, 57, 8, 866, 13, 1783, 24, 65, 118, 612, 976, 3, 88, 243, 5, 96, 14116, 34, 19, 842, 18, 18087, 21, 151, 113, 43, 118, 5241, 91, 13, 70, 2503, 11, 8, 1113, 30, 1623, 535, 216, 243, 34, 47, 359, 24, 96, 603, 5700, 342, 2245, 121, 130, 1026, 12, 1822, 8, 844, 167, 9930, 11, 3, 9, 964, 97, 3869, 474, 16, 286, 21, 8347, 9793, 1390, 5, 2114, 25, 118, 4161, 57, 18368, 16, 970, 51, 89, 2593, 11, 10987, 32, 1343, 42, 8, 17600, 7, 58, 8779, 178, 81, 39, 351, 13, 8, 1419, 11, 149, 34, 47, 10298, 5, 8601, 178, 30, 142, 40, 157, 12546, 5, 15808, 1741, 115, 115, 75, 5, 509, 5, 1598, 42, 146, 51, 89, 2593, 1741, 115, 115, 75, 5, 509, 5, 1598, 5, 1], [21603, 10, 71, 1472, 6196, 877, 326, 44, 8, 9108, 86, 29, 16, 6000, 1887, 44, 81, 11484, 10, 1755, 272, 4209, 30, 1856, 11, 2554, 130, 1380, 12, 1175, 8, 1595, 5, 282, 79, 3, 9094, 1067, 79, 1509, 8, 192, 14264, 6, 3, 16669, 596, 18, 969, 18, 1583, 16, 8, 443, 2447, 6, 3, 35, 6106, 19565, 57, 12314, 7, 5, 555, 13, 8, 1552, 1637, 19, 45, 3434, 6, 8, 119, 45, 1473, 11, 14441, 5, 94, 47, 70, 166, 706, 16, 5961, 5316, 5, 37, 2535, 13, 80, 13, 8, 14264, 243, 186, 13, 8, 9234, 141, 646, 525, 12770, 7, 30, 1476, 11, 175, 141, 118, 10932, 5, 2867, 1637, 43, 13666, 3709, 11210, 11, 56, 1731, 70, 1552, 13, 8, 3457, 4939, 865, 145, 79, 141, 4355, 5, 5076, 43, 3958, 15, 26, 21, 251, 81, 8, 3211, 5, 86, 7, 102, 1955, 24723, 243, 10, 96, 196, 17, 3475, 38, 713, 8, 1472, 708, 365, 80, 13, 8, 14264, 274, 16436, 12, 8, 511, 5, 96, 27674, 8, 2883, 1137, 19, 341, 365, 4962, 6, 34, 19, 816, 24, 8, 1472, 47, 708, 24067, 535, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[7433, 18, 413, 2673, 33, 6168, 640, 8, 12580, 17600, 7, 11, 970, 51, 89, 2593, 11, 10987, 32, 1343, 227, 18368, 2953, 57, 16133, 4937, 5, 1], [2759, 8548, 14264, 43, 118, 10932, 57, 1472, 16, 3, 9, 18024, 1584, 739, 3211, 16, 27874, 690, 2050, 5, 1]]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test it using the below call\n",
        "preprocess_function(raw_datasets['train'][:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-6iXTkIrJT"
      },
      "source": [
        "This function can be applied to all our datasets using the `map` method of our `dataset` object. The results are automatically cached to avoid spending time on this step the next time you run your notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DDtsaJeVIrJT",
        "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-5ebb2d6a3db1bb19.arrow\n",
            "Loading cached processed dataset at /vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-933a7931c8603918.arrow\n",
            "Loading cached processed dataset at /vol/bitbucket/aeg19/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-6ce473e7288e00a1.arrow\n"
          ]
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(\n",
        "    preprocess_function, \n",
        "    batched=True            # This employs multithreading to speed up tokenization\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using T5 out-of-the-box\n",
        "\n",
        "In the lecture slides you were shown the below image:\n",
        "\n",
        "![T5 img](figs/t5.png)\n",
        "\n",
        "Here, we will experiment with some of T5's capabilities without pre-training. First, we will download the model using `AutoModelForSeq2SeqLM` class using the `from_pretrained` method (this caches the model for us).\n",
        "\n",
        "To illustrate the impact of the prompt, we will try T5 using two different prompts using the same string. Note that we are using the small version without fine-tuning so the results may not be great."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 512)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 8)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
              "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
              "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your task\n",
        "Complete the `tok_and_gen` function below. This requires the following steps:\n",
        "- Tokenize the inputs\n",
        "- Generate output ids (hint: using model.generate. Google this for more details)\n",
        "- Convert the ids to string\n",
        "- Print the string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using prefix translate English to German: :  ['Die künstliche allgemeine Intelligenz (AGI) ist die']\n",
            "Using prefix summarize: :  ['artificial intelligence is the hypothetical ability of an intelligent agent to understand or learn any intellectual task that ']\n"
          ]
        }
      ],
      "source": [
        "input_str = 'Artificial general intelligence (AGI) is the hypothetical ability of an intelligent agent to understand or learn any intellectual task that a human being can.[1] It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies.'\n",
        "\n",
        "prefixes = [\"translate English to German: \", \"summarize: \"]     # What happens if we make up a prompt? Why?\n",
        "\n",
        "def tok_and_gen(model, tokenizer, input, prefix=''):\n",
        "    ## YOUR CODE HERE ##\n",
        "    inputs = tokenizer(prefix + input, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"].cuda(),\n",
        "        attention_mask=inputs[\"attention_mask\"].cuda(),\n",
        "    )\n",
        "\n",
        "    print(f'Using prefix {task_prefix}: ', tokenizer.batch_decode(output_sequences, skip_special_tokens=True))\n",
        "\n",
        "\n",
        "for task_prefix in prefixes:\n",
        "    tok_and_gen(model, tokenizer, task_prefix, input_str)    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "545PP3o8IrJV"
      },
      "source": [
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBiW8UpKIrJW"
      },
      "source": [
        "Now that our data is ready and we have played around with our model, we can fine-tune it. \n",
        "\n",
        "HuggingFace provides an API for training a seq2seq model: the `Seq2SeqTrainer`. To instantiate this, we will need to define three more things. The most important is the [`Seq2SeqTrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Seq2SeqTrainingArguments), which is a class that contains all the attributes to customize the training. It requires a folder name for saving checkpoints of the model, and all other arguments are optional:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X8WyIhF-__sL"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned-xsum\",     # Ouptut folder\n",
        "    # Eval strategy\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    # Could alternatively be the following to eval every epoch:\n",
        "    # evaluation_strategy=\"epoch\",\n",
        "\n",
        "    # LR. Should be small (<1e-4)\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # Batch size during training and eval\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "\n",
        "    # Limits the number of models saved during training. Important to prevent memory clogging up!\n",
        "    save_total_limit=3,\n",
        "\n",
        "    # Properly generate summaries during eval\n",
        "    predict_with_generate=True,\n",
        "    \n",
        "    # Mixed precision training (speeds up training - see Nvidia-apex for more details)\n",
        "    fp16=True,\n",
        "\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3pGVdTIrJc"
      },
      "source": [
        "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_tMJC9gy__sL"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZOdRlRIrJd"
      },
      "source": [
        "The last thing to define for our `Seq2SeqTrainer` is how to compute the metrics from the predictions. We need to define a function for this, which will just use the `metric` we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UmvbnJ9JIrJd"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXuFTAzDIrJe"
      },
      "source": [
        "Then we just need to pass all of this along with our datasets to the `Seq2SeqTrainer`. This is the general Huggingface API adapted for training seq2seq models.\n",
        "\n",
        "## Your task \n",
        "- Instantiate the trainer class and begin finetuning on the XSum dataset. You may find [this page](https://huggingface.co/docs/transformers/main_classes/trainer) helpful.\n",
        "\n",
        "**Note:** training will likely take a while so you may want to end training once you are satisfied it is working correctly in order to progress to the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "imY1oC3SIrJf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using amp half precision backend\n"
          ]
        }
      ],
      "source": [
        "## YOUR CODE HERE ##\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complexity Analysis\n",
        "\n",
        "Here we will analyze the computational complexity of the Transformer. One of the main drawbacks of this architecture is that the time and memory complexity scales poorly with respect to input length. This is particularly problematic for summarization as long document summarization can become prohibitively expensive.\n",
        "\n",
        "---------------- Questions/tasks ----------------\n",
        "\n",
        "1. Which component of the Transformer does the poor complexity problems mentioned above referred to? Explain why this is the case.\n",
        "2. We will profile the time complexity of T5 for different input sequence lengths. Complete the function below which will plot the time of T5's forward pass. You can use the inbuilt pytorch profiler or a simpler method (e.g. `time.time`) as you prefer.\n",
        "- We will first do this up to 512 tokens on a log base 2 scale (i.e. 1, 2, 4, 8, ... 512)\n",
        "- Do this for the encoder (holding decoder input length fixed) and the decoder (holding encoder input length fixed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
        "model.cuda()\n",
        "import torch\n",
        "\n",
        "\n",
        "from time import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def profile_seq2seq(model, max_len, batch_size=1):\n",
        "    # Create dummy input tensors\n",
        "    base_encoder_input = torch.tensor([[10]]).cuda()\n",
        "    base_decoder_input = torch.tensor([[10]]).cuda()\n",
        "    # Make a larger batch to make trends more evident\n",
        "    base_encoder_input = base_encoder_input.repeat(batch_size,1)\n",
        "    base_decoder_input = base_decoder_input.repeat(batch_size,1)\n",
        "    \n",
        "    ## YOUR CODE HERE ##\n",
        "    # Benchmark encoder holding decoder fixed\n",
        "    run_time_profile(model, max_len, base_encoder_input, base_decoder_input, 'encoder', True)\n",
        "\n",
        "    # Benchmark decoder holding encoder fixed\n",
        "    run_time_profile(model, max_len, base_encoder_input, base_decoder_input, 'decoder', True)\n",
        "\n",
        "\n",
        "def run_time_profile(model, max_len, base_encoder_input, base_decoder_input, vary, do_plot=False, n_trials=2):\n",
        "    times = []\n",
        "    for i in range(torch.log2(torch.tensor(max_len)).int()+1):\n",
        "        len = 2 ** i\n",
        "        \n",
        "        encoder_input = base_encoder_input\n",
        "        decoder_input = base_decoder_input\n",
        "        if vary == 'encoder':\n",
        "           encoder_input = base_encoder_input.repeat(1,len)\n",
        "        elif vary == 'decoder':\n",
        "           decoder_input = base_decoder_input.repeat(1,len) \n",
        "\n",
        "        # print(encoder_input.shape, decoder_input.shape)\n",
        "        ts = time()\n",
        "        for i in range(n_trials):\n",
        "            model(input_ids=encoder_input, decoder_input_ids=decoder_input)\n",
        "        te = time()\n",
        "        times.append(te - ts)\n",
        "\n",
        "    if do_plot:\n",
        "        plt.plot(times)\n",
        "        plt.show()\n",
        "\n",
        "    return times\n",
        "\n",
        "\n",
        "profile_seq2seq(model, max_len, 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modifying T5 to use longer sequences\n",
        "\n",
        "The above plots may or may not show an upward trajectory. Now try again with a larger batch size (e.g. 8). Now they should show an upward slope as the forward pass becomes more expensive relative to other overheads.\n",
        "\n",
        "The upward trajectory shows that the forward pass becomes more expensive as the sequence gets longer. However, we are still operating with short sequences. This trend will become clear for batch_size=1 if we are able to use longer sequences, but we are unable to exceed 512 tokens as this would exceed T5's embedding size.\n",
        "\n",
        "Below we will modify T5 to allow it to ingest longer sequences. This involves repeating the embedding matrix horizontally to the desired maximum length. Currently T5's embedding matrix is 512 tokens wide (including one reserved for the special token). Here we will extend the encoder's embedding matrix 8 times to 4089 tokens (511*8 + 1 for the special token). **Note** this technique is sometimes used in practise for long sequence tasks, but the model should be fine-tuned as the positial encodings in the repeated sections will not be optimal.\n",
        "\n",
        "### Your task\n",
        "\n",
        "Complete the `extend_embeddings` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  27,  114,    1],\n",
              "        [  71, 2701,    1]]), 'attention_mask': tensor([[1, 1, 1],\n",
              "        [1, 1, 1]])}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def extend_embeddings(model, widen_factor):\n",
        "    embed_size = model.encoder.embed_tokens\n",
        "    # allocate a larger position embedding matrix for the encoder\n",
        "    new_encoder_pos_embed = model.model.encoder.embed_positions.weight.new_empty(max_pos, embed_size)\n",
        "    # copy position embeddings over and over to initialize the new position embeddings\n",
        "    k = 2\n",
        "    step = current_max_pos - 2\n",
        "    while k < max_pos - 1:\n",
        "        new_encoder_pos_embed[k:(k + step)] = model.model.encoder.embed_positions.weight[2:]\n",
        "        k += step\n",
        "    model.model.encoder.embed_positions.weight.data = new_encoder_pos_embed"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Summarization",
      "provenance": []
    },
    "interpreter": {
      "hash": "65bdeda9c857d200db339bc1461d10a02d342df00d855a84b263b8efa6d3dc02"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('NLPlabs': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
